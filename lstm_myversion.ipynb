{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from utilities import *\n",
    "from keras import Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, Dense, GlobalMaxPooling1D, Embedding\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from file data/metadata.pkl.\n",
      "Loaded data from file embeddings/word2vec_GoogleNews_300dim.pkl.\n",
      "Loaded data from file data/train_data.pkl.\n",
      "Loaded data from file data/test_data.pkl.\n",
      "Loaded data from file data/val_data.pkl.\n",
      "Embeddings Model - Epochs=10 Hidden Layers=128\n",
      "------------------------------------\n",
      "Using parameters...\n",
      "Vocabulary size:  23103\n",
      "Number of labels:  41\n",
      "Embeddings dimension:  300\n",
      "Batch size:  200\n",
      "Hidden layer size:  128\n",
      "learning rate:  0.001\n",
      "Epochs:  10\n"
     ]
    }
   ],
   "source": [
    "resource_dir = 'data/'\n",
    "embeddings_dir = \"embeddings/\"\n",
    "embedding_filename = 'word2vec_GoogleNews'\n",
    "model_dir = 'models/'\n",
    "model_name = \"Embeddings Model\"\n",
    "\n",
    "# Load metadata\n",
    "metadata = load_data(resource_dir + \"metadata.pkl\")\n",
    "embeddings_dimension = 300\n",
    "embeddings = load_data(embeddings_dir + embedding_filename + '_' + str(embeddings_dimension) + 'dim.pkl')\n",
    "\n",
    "# Load Training and test sets\n",
    "train_data = load_data(resource_dir + 'train_data.pkl')\n",
    "train_x, train_y = generate_embeddings(train_data, metadata)\n",
    "train_y=np.array(np.nonzero(train_y))[1] #####diko m\n",
    "\n",
    "test_data = load_data(resource_dir + 'test_data.pkl')\n",
    "test_x, test_y = generate_embeddings(test_data, metadata)\n",
    "test_y=np.array(np.nonzero(test_y))[1]   #####diko m\n",
    "\n",
    "val_data = load_data(resource_dir + 'val_data.pkl')\n",
    "val_x, val_y = generate_embeddings(val_data, metadata)\n",
    "val_y=np.array(np.nonzero(val_y))[1]  #####diko m\n",
    "\n",
    "train_lengths = np.array(list(map(len, [sent for sent in train_data['utterances']]))) #####diko m\n",
    "valid_lengths = np.array(list(map(len, [sent for sent in val_data['utterances']]))) #####diko m\n",
    "\n",
    "# Parameters\n",
    "vocabulary_size = metadata['vocabulary_size']\n",
    "num_labels = metadata['num_labels']\n",
    "max_utterance_len = metadata['max_utterance_len']\n",
    "embedding_matrix = embeddings['embedding_matrix']\n",
    "batch_size = 200\n",
    "hidden_layer = 128\n",
    "learning_rate = 0.001\n",
    "num_epoch = 10\n",
    "#model_name = model_name + \" -\" + \\\n",
    "#             \" Epochs=\" + str(num_epoch) + \\\n",
    "#             \" Hidden Layers=\" + str(hidden_layer)\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Using parameters...\")\n",
    "print(\"Vocabulary size: \", vocabulary_size)\n",
    "print(\"Number of labels: \", num_labels)\n",
    "print(\"Embeddings dimension: \", embeddings_dimension)\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Hidden layer size: \", hidden_layer)\n",
    "print(\"learning rate: \", learning_rate)\n",
    "print(\"Epochs: \", num_epoch)\n",
    "\n",
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn_clf(object):\n",
    "    \"\"\"\"\n",
    "    LSTM and Bi-LSTM classifiers for text classification\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.num_classes = config.num_classes\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_layers = config.num_layers\n",
    "        self.l2_reg_lambda = config.l2_reg_lambda\n",
    "\n",
    "        # Placeholders\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[], name='batch_size')\n",
    "        self.input_x = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_x')\n",
    "        self.input_y = tf.placeholder(dtype=tf.int64, shape=[None], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name='keep_prob')\n",
    "        self.sequence_length = tf.placeholder(dtype=tf.int32, shape=[None], name='sequence_length')\n",
    "\n",
    "        # L2 loss\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Word embedding\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "            embedding = tf.get_variable('embedding',\n",
    "                                        shape=[self.vocab_size, self.hidden_size],\n",
    "                                        dtype=tf.float32)\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "\n",
    "        # Input dropout\n",
    "        self.inputs = tf.nn.dropout(inputs, keep_prob=self.keep_prob)\n",
    "\n",
    "        # LSTM\n",
    "        if config.clf == 'lstm':\n",
    "            self.final_state = self.normal_lstm()\n",
    "        else:\n",
    "            self.final_state = self.bi_lstm()\n",
    "\n",
    "        # Softmax output layer\n",
    "        with tf.name_scope('softmax'):\n",
    "            # softmax_w = tf.get_variable('softmax_w', shape=[self.hidden_size, self.num_classes], dtype=tf.float32)\n",
    "            if config.clf == 'lstm':\n",
    "                softmax_w = tf.get_variable('softmax_w', shape=[self.hidden_size, self.num_classes], dtype=tf.float32)\n",
    "            else:\n",
    "                softmax_w = tf.get_variable('softmax_w', shape=[2 * self.hidden_size, self.num_classes], dtype=tf.float32)\n",
    "            softmax_b = tf.get_variable('softmax_b', shape=[self.num_classes], dtype=tf.float32)\n",
    "\n",
    "            # L2 regularization for output layer\n",
    "            self.l2_loss += tf.nn.l2_loss(softmax_w)\n",
    "            self.l2_loss += tf.nn.l2_loss(softmax_b)\n",
    "\n",
    "            # self.logits = tf.matmul(self.final_state[self.num_layers - 1].h, softmax_w) + softmax_b\n",
    "            if config.clf == 'lstm':\n",
    "                self.logits = tf.matmul(self.final_state[self.num_layers - 1].h, softmax_w) + softmax_b\n",
    "                \n",
    "            else:\n",
    "                self.logits = tf.matmul(self.final_state, softmax_w) + softmax_b\n",
    "            predictions = tf.nn.softmax(self.logits)\n",
    "            self.predictions = tf.argmax(predictions, 1, name='predictions')\n",
    "\n",
    "        # Loss\n",
    "        with tf.name_scope('loss'):\n",
    "            tvars = tf.trainable_variables()\n",
    "\n",
    "            # L2 regularization for LSTM weights\n",
    "            for tv in tvars:\n",
    "                if 'kernel' in tv.name:\n",
    "                    self.l2_loss += tf.nn.l2_loss(tv)\n",
    "\n",
    "\n",
    "                    #i changed from sparse softmax to softmax otherwise i get error\n",
    "            losses =  tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y,\n",
    "                                                                    logits=self.logits)\n",
    "            self.cost = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, self.input_y)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "\n",
    "    def normal_lstm(self):\n",
    "        # LSTM Cell\n",
    "        cell = tf.contrib.rnn.LSTMCell(self.hidden_size,\n",
    "                                       forget_bias=1.0,\n",
    "                                       state_is_tuple=True,\n",
    "                                       reuse=tf.get_variable_scope().reuse)\n",
    "        # Add dropout to cell output\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "\n",
    "        # Stacked LSTMs\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([cell] * self.num_layers, state_is_tuple=True)\n",
    "\n",
    "        self._initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        # Dynamic LSTM\n",
    "        with tf.variable_scope('LSTM'):\n",
    "            outputs, state = tf.nn.dynamic_rnn(cell,\n",
    "                                               inputs=self.inputs,\n",
    "                                               initial_state=self._initial_state,\n",
    "                                               sequence_length=self.sequence_length)\n",
    "\n",
    "        final_state = state\n",
    "\n",
    "        return final_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, labels, lengths, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    A mini-batch iterator to generate mini-batches for training neural network\n",
    "    :param data: a list of sentences. each sentence is a vector of integers\n",
    "    :param labels: a list of labels\n",
    "    :param batch_size: the size of mini-batch\n",
    "    :param num_epochs: number of epochs\n",
    "    :return: a mini-batch iterator\n",
    "    \"\"\"\n",
    "    assert len(data) == len(labels) == len(lengths)\n",
    "\n",
    "    data_size = len(data)\n",
    "    epoch_length = data_size // batch_size\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for i in range(epoch_length):\n",
    "            start_index = i * batch_size\n",
    "            end_index = start_index + batch_size\n",
    "\n",
    "            xdata = data[start_index: end_index]\n",
    "            ydata = labels[start_index: end_index]\n",
    "            sequence_length = lengths[start_index: end_index]\n",
    "\n",
    "            yield xdata, ydata, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "batch_size: <absl.flags._flag.Flag object at 0x00000240063C9208>\n",
      "clf: <absl.flags._flag.Flag object at 0x000002400ED57898>\n",
      "data_file: <absl.flags._flag.Flag object at 0x000002400ED57358>\n",
      "embedding_size: <absl.flags._flag.Flag object at 0x000002400ED57048>\n",
      "evaluate_every_steps: <absl.flags._flag.Flag object at 0x000002401FBE3C88>\n",
      "f: <absl.flags._flag.Flag object at 0x000002401FBE3CC0>\n",
      "filter_sizes: <absl.flags._flag.Flag object at 0x000002400ED57F28>\n",
      "hidden_size: <absl.flags._flag.Flag object at 0x000002400ED57080>\n",
      "keep_prob: <absl.flags._flag.Flag object at 0x000002400ED57550>\n",
      "l2_reg_lambda: <absl.flags._flag.Flag object at 0x00000240063C95F8>\n",
      "language: <absl.flags._flag.Flag object at 0x000002400ED57B00>\n",
      "learning_rate: <absl.flags._flag.Flag object at 0x00000240063C93C8>\n",
      "max_length: <absl.flags._flag.Flag object at 0x000002400ED573C8>\n",
      "min_frequency: <absl.flags._flag.Flag object at 0x000002400ED57E80>\n",
      "num_checkpoint: <absl.flags._flag.Flag object at 0x000002401FBE3390>\n",
      "num_classes: <absl.flags._flag.Flag object at 0x000002400ED57320>\n",
      "num_epochs: <absl.flags._flag.Flag object at 0x00000240063C96D8>\n",
      "num_filters: <absl.flags._flag.Flag object at 0x000002400ED570B8>\n",
      "num_layers: <absl.flags._flag.Flag object at 0x000002400ED57438>\n",
      "save_every_steps: <absl.flags._flag.Flag object at 0x000002401FBE3B38>\n",
      "stop_word_file: <absl.flags._flag.Flag object at 0x000002400ED57A20>\n",
      "test_size: <absl.flags._flag.Flag object at 0x000002400ED574E0>\n",
      "vocab_size: <absl.flags._flag.Flag object at 0x000002400ED57A90>\n",
      "\n",
      "Start training ...\n",
      "2019-03-05T23:21:00.346637: step: 1, loss: 3.8717, accuracy: 0.135\n",
      "2019-03-05T23:21:00.637625: step: 2, loss: 3.81989, accuracy: 0.125\n",
      "2019-03-05T23:21:00.921901: step: 3, loss: 3.84224, accuracy: 0.18\n",
      "2019-03-05T23:21:01.250406: step: 4, loss: 3.81196, accuracy: 0.145\n",
      "2019-03-05T23:21:01.521457: step: 5, loss: 3.81513, accuracy: 0.32\n",
      "2019-03-05T23:21:01.854480: step: 6, loss: 3.75015, accuracy: 0.52\n",
      "2019-03-05T23:21:02.070337: step: 7, loss: 3.77771, accuracy: 0.44\n",
      "2019-03-05T23:21:02.451394: step: 8, loss: 3.6621, accuracy: 0.635\n",
      "2019-03-05T23:21:02.759436: step: 9, loss: 3.6504, accuracy: 0.525\n",
      "2019-03-05T23:21:03.075516: step: 10, loss: 3.59186, accuracy: 0.495\n",
      "2019-03-05T23:21:03.285270: step: 11, loss: 3.46406, accuracy: 0.53\n",
      "2019-03-05T23:21:03.552602: step: 12, loss: 3.3457, accuracy: 0.59\n",
      "2019-03-05T23:21:03.914751: step: 13, loss: 3.32737, accuracy: 0.315\n",
      "2019-03-05T23:21:04.232858: step: 14, loss: 2.91494, accuracy: 0.34\n",
      "2019-03-05T23:21:04.535044: step: 15, loss: 3.02197, accuracy: 0.335\n",
      "2019-03-05T23:21:04.847702: step: 16, loss: 2.68342, accuracy: 0.34\n",
      "2019-03-05T23:21:05.145682: step: 17, loss: 2.65902, accuracy: 0.335\n",
      "2019-03-05T23:21:05.459111: step: 18, loss: 2.55478, accuracy: 0.355\n",
      "2019-03-05T23:21:05.829694: step: 19, loss: 2.20927, accuracy: 0.375\n",
      "2019-03-05T23:21:06.120639: step: 20, loss: 2.60006, accuracy: 0.35\n",
      "2019-03-05T23:21:06.418826: step: 21, loss: 2.29308, accuracy: 0.285\n",
      "2019-03-05T23:21:06.841598: step: 22, loss: 2.51494, accuracy: 0.285\n",
      "2019-03-05T23:21:07.201636: step: 23, loss: 2.70687, accuracy: 0.245\n",
      "2019-03-05T23:21:07.657646: step: 24, loss: 2.42338, accuracy: 0.34\n",
      "2019-03-05T23:21:07.935332: step: 25, loss: 2.66584, accuracy: 0.28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-f4cd0ce6491f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain_input\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-f4cd0ce6491f>\u001b[0m in \u001b[0;36mrun_step\u001b[1;34m(input_data, is_training)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mvars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cost'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####Delete all flags before declare#####\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import data_helper\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as e:\n",
    "    error = \"Please install scikit-learn.\"\n",
    "    print(str(e) + ': ' + error)\n",
    "    sys.exit()\n",
    "\n",
    "# Show warnings and errors only\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Parameters\n",
    "# =============================================================================\n",
    "\n",
    "# Model choices\n",
    "tf.flags.DEFINE_string('clf', 'lstm', \"Type of classifiers. Default: cnn. You have four choices: [cnn, lstm, blstm, clstm]\")\n",
    "\n",
    "# Data parameters\n",
    "tf.flags.DEFINE_string('data_file', 'train_data.pkl', 'Data file path')\n",
    "tf.flags.DEFINE_string('stop_word_file', None, 'Stop word file path')\n",
    "tf.flags.DEFINE_string('language', 'en', \"Language of the data file. You have two choices: [ch, en]\")\n",
    "tf.flags.DEFINE_integer('min_frequency',0, 'Minimal word frequency')\n",
    "tf.flags.DEFINE_integer('num_classes', 41, 'Number of classes')\n",
    "tf.flags.DEFINE_integer('max_length', 106, 'Max document length')\n",
    "tf.flags.DEFINE_integer('vocab_size', 23103, 'Vocabulary size')\n",
    "tf.flags.DEFINE_float('test_size', 0.1, 'Cross validation test size')\n",
    "\n",
    "# Model hyperparameters\n",
    "tf.flags.DEFINE_integer('embedding_size', 300, 'Word embedding size. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_string('filter_sizes', '3, 4, 5', 'CNN filter sizes. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_integer('num_filters', 128, 'Number of filters per filter size. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_integer('hidden_size', 128, 'Number of hidden units in the LSTM cell. For LSTM, Bi-LSTM')\n",
    "tf.flags.DEFINE_integer('num_layers', 2, 'Number of the LSTM cells. For LSTM, Bi-LSTM, C-LSTM')\n",
    "tf.flags.DEFINE_float('keep_prob', 0.5, 'Dropout keep probability')  # All\n",
    "tf.flags.DEFINE_float('learning_rate', 1e-3, 'Learning rate')  # All\n",
    "tf.flags.DEFINE_float('l2_reg_lambda', 0.001, 'L2 regularization lambda')  # All\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer('batch_size', 200, 'Batch size')\n",
    "tf.flags.DEFINE_integer('num_epochs', 10, 'Number of epochs')\n",
    "tf.flags.DEFINE_integer('evaluate_every_steps', 100, 'Evaluate the model on validation set after this many steps')\n",
    "tf.flags.DEFINE_integer('save_every_steps', 1000, 'Save the model after this many steps')\n",
    "tf.flags.DEFINE_integer('num_checkpoint', 10, 'Number of models to store')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "if FLAGS.clf == 'lstm':\n",
    "    FLAGS.embedding_size = FLAGS.hidden_size\n",
    "elif FLAGS.clf == 'clstm':\n",
    "    FLAGS.hidden_size = len(FLAGS.filter_sizes.split(\",\")) * FLAGS.num_filters\n",
    "\n",
    "# Output files directory\n",
    "timestamp = str(int(time.time()))\n",
    "outdir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# Load and save data\n",
    "# =============================================================================\n",
    "\n",
    "#data, labels, lengths, vocab_processor = data_helper.load_data(file_path=FLAGS.data_file,\n",
    "#                                                               sw_path=FLAGS.stop_word_file,\n",
    "#                                                               min_frequency=FLAGS.min_frequency,\n",
    "#                                                               max_length=FLAGS.max_length,\n",
    "#                                                               language=FLAGS.language,\n",
    "#                                                               shuffle=True)\n",
    "\n",
    "# Save vocabulary processor\n",
    "#vocab_processor.save(os.path.join(outdir, 'vocab'))\n",
    "\n",
    "#FLAGS.vocab_size = len(vocab_processor.vocabulary_._mapping)\n",
    "\n",
    "#FLAGS.max_length = vocab_processor.max_document_length\n",
    "\n",
    "params = FLAGS.__flags\n",
    "# Print parameters\n",
    "model = params['clf']\n",
    "if model == 'cnn':\n",
    "    del params['hidden_size']\n",
    "    del params['num_layers']\n",
    "elif model == 'lstm' or model == 'blstm':\n",
    "    del params['num_filters']\n",
    "    del params['filter_sizes']\n",
    "    params['embedding_size'] = params['hidden_size']\n",
    "elif model == 'clstm':\n",
    "    params['hidden_size'] = len(list(map(int, params['filter_sizes'].split(\",\")))) * params['num_filters']\n",
    "\n",
    "params_dict = sorted(params.items(), key=lambda x: x[0])\n",
    "print('Parameters:')\n",
    "for item in params_dict:\n",
    "    print('{}: {}'.format(item[0], item[1]))\n",
    "print('')\n",
    "\n",
    "# Save parameters to file\n",
    "#params_file = open(os.path.join(outdir, 'params.pkl'), 'wb')\n",
    "#pkl.dump(params, params_file, True)\n",
    "#params_file.close()\n",
    "\n",
    "\n",
    "# Simple Cross validation\n",
    "# TODO use k-fold cross validation\n",
    "#x_train, x_valid, y_train, y_valid, train_lengths, valid_lengths = train_test_split(data,\n",
    "#                                                                                    labels,\n",
    "#                                                                                    lengths,\n",
    "#                                                                                    test_size=FLAGS.test_size,\n",
    "#                                                                                    random_state=22)\n",
    "\n",
    "# Batch iterator\n",
    "data_train = batch_iter(train_x, train_y, train_lengths, FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "# Train\n",
    "# =============================================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        if FLAGS.clf == 'cnn':\n",
    "            classifier = cnn_clf(FLAGS)\n",
    "        elif FLAGS.clf == 'lstm' or FLAGS.clf == 'blstm':\n",
    "            classifier = rnn_clf(FLAGS)\n",
    "        elif FLAGS.clf == 'clstm':\n",
    "            classifier = clstm_clf(FLAGS)\n",
    "        else:\n",
    "            raise ValueError('clf should be one of [cnn, lstm, blstm, clstm]')\n",
    "\n",
    "        # Train procedure\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(classifier.cost)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Summaries\n",
    "        loss_summary = tf.summary.scalar('Loss', classifier.cost)\n",
    "        accuracy_summary = tf.summary.scalar('Accuracy', classifier.accuracy)\n",
    "\n",
    "        # Train summary\n",
    "        train_summary_op = tf.summary.merge_all()\n",
    "        train_summary_dir = os.path.join(outdir, 'summaries', 'train')\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Validation summary\n",
    "        valid_summary_op = tf.summary.merge_all()\n",
    "        valid_summary_dir = os.path.join(outdir, 'summaries', 'valid')\n",
    "        valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=FLAGS.num_checkpoint)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def run_step(input_data, is_training=True):\n",
    "            \"\"\"Run one step of the training process.\"\"\"\n",
    "            input_x, input_y, sequence_length = input_data\n",
    "            #input_y=np.transpose(np.array(np.nonzero(input_y)[1]))\n",
    "            fetches = {'step': global_step,\n",
    "                       'cost': classifier.cost,\n",
    "                       'accuracy': classifier.accuracy}\n",
    "            feed_dict = {classifier.input_x: input_x,\n",
    "                         classifier.input_y: input_y}\n",
    "\n",
    "            if FLAGS.clf != 'cnn':\n",
    "                fetches['final_state'] = classifier.final_state\n",
    "                feed_dict[classifier.batch_size] = len(input_x)\n",
    "                feed_dict[classifier.sequence_length] = sequence_length\n",
    "\n",
    "            if is_training:\n",
    "                fetches['train_op'] = train_op\n",
    "                fetches['summaries'] = train_summary_op\n",
    "                feed_dict[classifier.keep_prob] = FLAGS.keep_prob\n",
    "            else:\n",
    "                fetches['summaries'] = valid_summary_op\n",
    "                feed_dict[classifier.keep_prob] = 1.0\n",
    "\n",
    "            vars = sess.run(fetches, feed_dict)\n",
    "            step = vars['step']\n",
    "            cost = vars['cost']\n",
    "            accuracy = vars['accuracy']\n",
    "            summaries = vars['summaries']\n",
    "\n",
    "            # Write summaries to file\n",
    "            if is_training:\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "            else:\n",
    "                valid_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step: {}, loss: {:g}, accuracy: {:g}\".format(time_str, step, cost, accuracy))\n",
    "\n",
    "            return accuracy\n",
    "\n",
    "\n",
    "        print('Start training ...')\n",
    "\n",
    "        for train_input in data_train:\n",
    "            run_step(train_input, is_training=True)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "            if current_step % FLAGS.evaluate_every_steps == 0:\n",
    "                print('\\nValidation')\n",
    "                #val_y=np.array(np.nonzero(val_y)[1]) ###diko m\n",
    "                run_step((val_x, val_y, valid_lengths), is_training=False)\n",
    "                print('')\n",
    "\n",
    "            if current_step % FLAGS.save_every_steps == 0:\n",
    "                save_path = saver.save(sess, os.path.join(outdir, 'model/clf'), current_step)\n",
    "\n",
    "        print('\\nAll the files have been saved to {}\\n'.format(outdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x, val_y = generate_embeddings(val_data, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192768, 106)\n",
      "(192768,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = generate_embeddings(train_data, metadata)\n",
    "train_y=np.array(np.nonzero(train_y))[1] #####diko m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60  0  0 ...  0  0  0]\n",
      " [ 4  5  0 ...  0  0  0]\n",
      " [ 4  0  6 ...  0  0  0]\n",
      " ...\n",
      " [61  0 11 ...  0  0  0]\n",
      " [72 25 18 ...  0  0  0]\n",
      " [20  1  0 ...  0  0  0]]\n",
      "[ 1  3  2  1  3  0  0  1  0  1  0  0  1  2 15  1  2  1  2  0 20  1  3  3\n",
      "  0  9  0 21 29  3  2 27  4  4  0  0 14  7  0  0  1  0  1  3  2  0  0  0\n",
      "  1  0  0  2  2  1  2  4  3  2  4  2  4  3  0  2  2  1  2  2  4 16  4  2\n",
      "  3  9  3  2  9  3  2  0  2  1  5  2  4  4  3  2  4  2  2  1 17  3  2  1\n",
      " 22  6  6 28  0  0  0  0  0  0  1  0  0 14  7  3  0  0  3  5  0  0 20  4\n",
      "  4  3  2  4  2  4  1  3  3  0  0 17  0  0  0  0  0  0  0  0  0  0  0  5\n",
      "  1  0  0  2  0  0  1  0  0  0  1  2  2  1  2  1  2  1  2  0  0  1  0  0\n",
      "  1  0  1  0  1  2  1  2  1  2  1  2  1  2  4  4  2  1  1  2  2  2  1  1\n",
      "  0  0  0  0  0  0  0  1]\n",
      "[ 1  4 31  2  3  8 13  1 29  2 11 16  2 25 22  2 19  2 22  8  6  2 14  2\n",
      "  7  8  6  6  2 11 41  3  2  2 27 24  4  2 14 18  2 14  3  5 17  3  3 11\n",
      "  2  6 14 28  8  2  4  2  5 37  6 47  2 10 11 33 27  2 17 28  2  4  2  3\n",
      "  2 10  2 16  6  5 10 21 32  2  5 10  2  2  6 37  4 16 10  2  3  3  8  3\n",
      " 10  7  5  8 24  7 18 24 10 14  2 23  8  4  2 12  8 26  5  5 15 27  9  2\n",
      "  2  2 14  2 10  3  2  2  3 14 11  2 22 12 17 17  5 10  3 15 22 11  8  6\n",
      "  2 21 16 16  5  8  2  7 16 24  2 12 18  2 16  2 18  2  7 13 16  2 19  9\n",
      "  2 18  2 40  2 21  2 11  2 13  2 24  2  8  2  4 20  2  2 18 13 14  4  2\n",
      "  8  5 17  7  4 15 10  2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c=0\n",
    "for train_input in data_train: \n",
    "    if c==1:\n",
    "        break\n",
    "    input_x, input_y,lent = train_input\n",
    "  \n",
    "  #print(train_input)\n",
    "  #input_x,input_y=train_input\n",
    "    c=1\n",
    "    \n",
    "print(input_x) \n",
    "print(input_y)\n",
    "print(lent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
